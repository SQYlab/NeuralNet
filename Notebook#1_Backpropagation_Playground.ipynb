{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook \\#1 - Backpropagation Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [PART 1] Forward Pass: Single-Layer Neural Net "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P10 - Common Activations functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Activation functions:\n",
    "\n",
    "def sig(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def tanh(z):\n",
    "    return (1 - np.exp(-2*z)) / (1 + np.exp(-2*z)) \n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "# Derivative Activation functions:\n",
    "\n",
    "def d_sig(z):\n",
    "    return (1 - sig(z)) * sig(z)\n",
    "\n",
    "def d_tanh(z):\n",
    "    return 1 - tanh(z)**2\n",
    "\n",
    "def d_relu(z):\n",
    "    return np.where(z<0, z, 1).clip(min=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P11 - Computing activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(2)*2                        # input vector\n",
    "\n",
    "# FORWARD PASS =======================\n",
    "# f = sigmoid(dot) \n",
    "# with dot = w.x\n",
    "\n",
    "w = np.array([[2, -3], [1, 2], [0.5, -0.1]])   # 3 neurons (weigths)\n",
    "dot = np.dot(w, x)\n",
    "act = sig(dot)\n",
    "\n",
    "# ------------------------------------\n",
    "print \"INPUTS : x =\", x\n",
    "print \"\\nNEURAL NET : weights:\\n\", w\n",
    "print \"\\nDOT PRODUCT:\", dot\n",
    "print \"\\nACTIVATIONS (SIGMOID):\", act\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P12 - Plotting the activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting:\n",
    "zs = [(-10+i/10.0) for i in range(200)]\n",
    "\n",
    "plt.plot(zs, [sig(i) for i in zs], label=\"${\\sigma}(x)$\")\n",
    "plt.plot(zs, [d_sig(i) for i in zs], label=\"$d{\\sigma}(x)$\")\n",
    "plt.plot(dot, act, 'ro', label=\"neurons outputs\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid Function $\\sigma$(x)\\nNeurons Activations\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"activations\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(zs, [tanh(i) for i in zs], label=\"tanh(x)\")\n",
    "plt.plot(zs, [d_tanh(i) for i in zs], '--', label=\"d_tanh(x)\")\n",
    "plt.plot(dot, tanh(dot), 'ro', label=\"neurons outputs\")\n",
    "plt.title(\"tanh Function\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"activations\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(zs, [relu(i) for i in zs], label=\"relu(x)\")\n",
    "plt.plot(zs, [d_relu(i) for i in zs], '--', label=\"d_relu(x)\")\n",
    "plt.plot(dot, relu(dot), 'ro', label=\"neurons outputs\")\n",
    "plt.title(\"relu Function\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"activations\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [PART 2] Backpropagation : An Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1:\n",
    "\n",
    "Compute the gradient of the following function using *Automatic Differentiation*: \n",
    "\n",
    "$f(x, y) = \\frac{x + {\\sigma}(y)}{{\\sigma}(x) + (x + y)^2} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x,y):\n",
    "    sig_y = sig(y)                     # (1) : sig\n",
    "    sig_x = sig(x)                     # (2) : sig\n",
    "    sum_xy = x + y                     # (3) : sum\n",
    "    sq_sum_xy = sum_xy**2              # (4) : squarre\n",
    "    num = x + sig_y                    # (5) : sum\n",
    "    den = sig_x + sq_sum_xy            # (6) : sum\n",
    "    inv_den = 1 / den                  # (7) : inverse\n",
    "    f = num * inv_den                  # (8) : multiplication\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f(3,-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradf(x,y):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the function f(x,y).\n",
    "    \"\"\"\n",
    "    \n",
    "    # forward pass:\n",
    "\n",
    "    sig_y = sig(y)                     # (1) : sig\n",
    "    sig_x = sig(x)                     # (2) : sig\n",
    "    sum_xy = x + y                     # (3) : sum\n",
    "    sq_sum_xy = sum_xy**2              # (4) : squarre\n",
    "    num = x + sig_y                    # (5) : sum\n",
    "    den = sig_x + sq_sum_xy            # (6) : sum\n",
    "    inv_den = 1 / den                  # (7) : inverse\n",
    "    f = num * inv_den                  # (8) : multiplication\n",
    "    \n",
    "    # backprop pass:\n",
    "\n",
    "    df_dnum       = inv_den           # (8)\n",
    "    df_dinv       = num               # (8)\n",
    "    dinv_dden     = -1/(den**2)       # (7)\n",
    "    dden_dsigx    = 1                 # (6)\n",
    "    dden_dsq      = 1                 # (6)\n",
    "    dnum_dx       = 1                 # (5)\n",
    "    dnum_dsigy    = 1                 # (5)\n",
    "    dsq_dsumxy    = 2*sum_xy          # (4)\n",
    "    dsumxy_dx     = 1                 # (3)\n",
    "    dsumxy_dy     = 1                 # (3)\n",
    "    dsigx_dx      = d_sig(x)          # (2)\n",
    "    dsigy_dy      = d_sig(y)          # (1)\n",
    "\n",
    "    dx = df_dnum*dnum_dx + df_dinv*dinv_dden*(dden_dsigx*dsigx_dx + dden_dsq*dsq_dsumxy*dsumxy_dx)\n",
    "    dy = df_dnum*dnum_dsigy*dsigy_dy + df_dinv*dinv_dden*dden_dsq*dsq_dsumxy*dsumxy_dy\n",
    "    return [dx, dy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradf_v2(x,y):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the function f(x,y).\n",
    "    Same as before but a bit different.\n",
    "    \"\"\"\n",
    "    \n",
    "    # forward pass:\n",
    "\n",
    "    sig_y = sig(y)                     # (1) : sig\n",
    "    sig_x = sig(x)                     # (2) : sig\n",
    "    sum_xy = x + y                     # (3) : sum\n",
    "    sq = sum_xy**2                     # (4) : squarre\n",
    "    num = x + sig_y                    # (5) : sum\n",
    "    den = sig_x + sq                   # (6) : sum\n",
    "    inv_den = 1 / den                  # (7) : inverse\n",
    "    f = num * inv_den                  # (8) : multiplication\n",
    "    \n",
    "    # my backprop:\n",
    "\n",
    "    df_dx,df_dy   = 0,0                               # init\n",
    "    df_dnum       = 1.0 * inv_den                     # (8) mult\n",
    "    df_dinv       = 1.0 * num                         # (8) mult\n",
    "    df_dden       = df_dinv * -1/(den**2)             # (7) inv.\n",
    "    df_dsigx      = df_dden * 1                       # (6) sum\n",
    "    df_dsq        = df_dden * 1                       # (6) sum\n",
    "    df_dx        += df_dnum * 1                       # (5) sum\n",
    "    df_dsigy      = df_dnum * 1                       # (5) sum\n",
    "    df_dsumxy     = df_dsq  * 2 * sum_xy              # (4) square \n",
    "    df_dx        += df_dsumxy * 1                     # (3) sum\n",
    "    df_dy        += df_dsumxy * 1                     # (3) sum\n",
    "    df_dx        += df_dsigx * d_sig(x)               # (2) sig\n",
    "    df_dy        += df_dsigy * d_sig(y)               # (1) sig\n",
    "\n",
    "    return [df_dx, df_dy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing - Compute the gradient of f(x,y) at (3,-4):\n",
    "print gradf(3,-4)\n",
    "print gradf_v2(3,-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zs = [(-10+i/10.0) for i in range(300)]\n",
    "plt.plot(zs, [f(i,-4) for i in zs], label='y= -4')\n",
    "plt.plot(zs, [f(i,-3) for i in zs], label='y= -3')\n",
    "plt.plot(zs, [f(i,-2) for i in zs], label='y= -2')\n",
    "plt.plot(zs, [f(i,-1) for i in zs], label='y= -1')\n",
    "plt.plot(zs, [f(i, 0) for i in zs], label='y=  0')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"$f(x,y)$\")\n",
    "plt.title(\"$f(x,y)$ for few y values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fig = plt.figure()\n",
    "#ax = Axes3D(fig)\n",
    "# plot the 3D curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "y = np.random.randn()\n",
    "zs = [(-10+i/10.0) for i in range(300)]\n",
    "plt.plot(zs, [f(i, y) for i in zs], label=\"y= {:.2f} \".format(y))\n",
    "plt.plot(zs, [gradf(i, y)[0] for i in zs], '--', label=\"derivative\")\n",
    "plt.plot(3, f(3, y), 'or')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"$f(x,y)$\")\n",
    "plt.title(\"$f(x,y)$ for one y values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2: \n",
    "Backprop of a the Single-Layer Neural Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_sigmoid(W, x):\n",
    "    # forward:\n",
    "    z = W.dot(x)\n",
    "    E = np.exp(-z)\n",
    "    X = E + 1\n",
    "    f = 1 / X\n",
    "    \n",
    "    # backprop:\n",
    "    dX = -1 / X**2              # df_dX\n",
    "    dE = 1 * dX                 # (df_dX) * dX_dE\n",
    "    dz = -E * dE                # (df_dX * dE_dE) * dE_dz\n",
    "    \n",
    "    # can be simplify by doing dz = d_sig(z)\n",
    "    \n",
    "    x = x.reshape(1,-1)\n",
    "    dz = dz.reshape(-1, 1)\n",
    "    dW = dz.dot(x)              # (df_dX * dE_dE * dE_dz) * dz_dW\n",
    "    \n",
    "    return [f, dW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "W = np.array([[2, -3], [1, 2], [0.5, -0.1]])               \n",
    "x = np.random.randn(2)*2                                   \n",
    "\n",
    "acts, grads = grad_sigmoid(W, x)\n",
    "print \"ACTIVATIONS:\", acts\n",
    "print \"\\nGRADS:\\n\", grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Activations:\", acts\n",
    "print \"Update:\"\n",
    "\n",
    "for i in range(20):\n",
    "    # update (gradient ascend):\n",
    "    # show that the updates makes all the activations increase towards 1.\n",
    "    W = W + 1.0 * grads\n",
    "    acts, grads = grad_sigmoid(W, x)\n",
    "    print \"step {:3d}\".format(i+1),\":\",acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
